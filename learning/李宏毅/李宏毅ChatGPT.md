# ChatGPT剖析原理及应用

https://www.bilibili.com/video/BV1WX4y1Q7W4

## GPT社会化过程

### GPT原理

官方只给了blog，没有给论文，但是提到了兄弟是[Instruct GPT](https://arxiv.org/abs/2203.02155)

下面的原理围绕着它展开，但是他们本身差距也非常小

![](images\2023-03-25-10-42-33-image.png)

![](images\2023-03-25-10-42-45-image.png)

### 学习四阶段

#### Step 1 文字接龙

不需要人工标注，只要搜集大量的文字

![](images\2023-03-25-10-44-05-image.png)

但是 你好 可以后面接很多字，其实GPT的输出是一个概率分布，然后从这里面做sample随机抽取，**所以每次的输出都不同！** 每次补的结果是有随机性的

![](images\2023-03-25-10-45-17-image.png)

学习文字接龙，就可以让 它来回答问题！

![](images\2023-03-25-10-46-03-image.png)

但学习文字接龙没有很好用，因为每次结果概率都不同。特别是它如果在网上看过一些选择题考卷，里面后面接了一些错误答案，所以需要引导！

![](images\2023-03-25-10-47-18-image.png)

#### Step 2 人类引导

GPT看过了网络的答案后，再看看人提供的一些有用的问句

instuct GPT没有用太多的标注答案，因为GPT本身就有答案，只是在第二阶段只要提供一些范例，引导他就可以了

![](images\2023-03-25-10-48-11-image.png)

#### Step 3 模仿人类老师的喜好

很多人使用OpenAI的API去问问题，把问题收集好，让GPT产生问题答案

接着去雇佣很多人去标注， 人类老师不需要提供正确答案，只是告诉机器哪个答案好，哪个答案差

接着让GPT去**训练一个老师的模型**，模仿人类老师的评判标准

![](images\2023-03-25-10-50-07-image.png)

#### Step 4 用增强式学习向模拟老师学习

问一个问题，可能接了一句有作用的话，但是这是问句，不是人类想要的

通过强化学习的技术，让老师模型给到Reward，不断训练收获高分

**这样的目的是训练Tearcher Model**

![](images\2023-03-25-10-56-08-image.png)

### 缺点和总结

![](images\2023-03-25-10-59-14-image.png)

所以说ChatGPT是GPT的社会化

<img src="file:///C:/Users/lyf/AppData/Roaming/marktext/images/2023-03-25-10-59-49-image.png" title="" alt="" data-align="right">

## ChatGPT原理剖析

### 对ChatGPT的常见误解

#### 案例

每次输出都不同 - 给课程规划

可以追问 - 让他优化可成

#### 误解1 罐头回应

并不是准备好了罐头准备回应

![](images\2023-03-25-11-03-32-image.png)

这个笑话完全不是人的笑点，不可能是人写的

![](images\2023-03-25-11-04-42-image.png)

这个稍微好一点

![](images\2023-03-25-11-05-39-image.png)

这个是最冷最好的

![](images\2023-03-25-11-06-39-image.png)

#### 误解2 网络搜寻

![](images\2023-03-25-11-07-37-image.png)

比如问念能力的网址，上面的网页没有一个存在，是它幻想出来的

![](images\2023-03-25-11-07-48-image.png)

官方提到它没有联网，对2021年后的事情所知有限

![](images\2023-03-25-11-08-35-image.png)

#### 真正做的事情 文字接龙

文字接龙，也隶属于语言模型

![](images\2023-03-25-11-11-19-image.png)

如何知道过去的内容？因为包含了过去所有对话的输入

ChatGPT应该比GPT3更大，GPT3是1700亿个参数

![](images\2023-03-25-11-12-34-image.png)

这个f怎么找出来的？

训练有用网络资料，找出f之后，就不需要联网了

就和考试一样，之前可以备考，测试是真的上考场了，就不能翻书了

![](images\2023-03-25-11-13-46-image.png)

### 预训练 Pre-train

Generateive **Pre-train** Transformer

#### 从有监督到无监督

机器通过我们标注后的语句，可能就可以学到词对应的中文含义

![](images\2023-03-26-22-37-45-image.png)

如果使用有监督学习套用在GPT，其能提供的资讯是很少的，所以我们得找个方法让机器无痛的制造成对资料

![](images\2023-03-26-22-39-41-image.png)

这种方法就是采用文字接龙

![](images\2023-03-26-22-40-27-image.png)

#### GPT模型的发展

##### GPT-2

在chatGPT之前的GPT，模型和训练数据集不断增加

![](images\2023-03-26-22-41-14-image.png)

当年GPT-2有个津津乐道的事例，你和GPT说科学家发现了独角兽，GPT-2就开始续写独角兽的神态，这在当时是很不可思议的进展

![](images\2023-03-26-22-41-48-image.png)

GPT-2也可以回答问题，也可以进行摘要

![](images\2023-03-26-22-45-12-image.png)

在CoQA这个问答集下，发现随着GPT-2模型的参数不断增大，其回答问题的正确率也越高，但是人类的表现是非常强的，GPT-2不过也不错，也比很多模型厉害

![](images\2023-03-26-22-45-46-image.png)

##### GPT-3

1年后，GPT-3出世，模型参数是GPT-2的100倍、数据是也是其10倍

GPT-3.5不是特指模型，官方说任何在GPT-3之上做的微调都较GPT3.5

![](images\2023-03-26-22-50-23-image.png)

GPT-3也是能写程序的，但是其还是有蛮大的局限性的

![](images\2023-03-26-22-53-22-image.png)

GPT-3论文中演示了42个任务的表现，参数量越多模型变小越好，但是模型极限也就到了50%左右

![](images\2023-03-26-22-54-57-image.png)

并且GPT-3是不受控制的，所以就需要强化GPT-3的能力

![](images\2023-03-26-22-55-18-image.png)

#### ChatGPT预训练

ChatGPT其实就是GPT-3增加老师学习的结果！在进行人类督导式学习之前，通过大量网络学习的过程，就叫做预训练。后面的继续学习过程，也就叫fine-tune。

预训练也叫自督导式学习，督导式学习是人类提供成对的资料去学习，自督导学习是通过无痛的方法去生成了成对的学习资料。因为ChatGPT是GPT产生出来的，所以GPT这种自督导式学习产生的模型给很多强大的应用提供了基础，所以也叫基石模型。所以自督导式学习和基石模型，讲的是同一件事情。而基石模型解释起来比较清楚，所以现在媒体都报道该词汇

![](images\2023-03-26-22-56-18-image.png)chatgpt是可以多语言的，有的说法是其背后有翻译模型，由于openai没有对其说明，所以不能排除这个可能，但是李宏毅老师怀疑是不需要用到翻译。很有可能老师只要教chatgpt几种语言，chatgpt就能自动学会其他语言

![](images\2023-03-26-23-01-01-image.png)

之前李宏毅实验室就发现过一个情况

![](images\2023-03-26-23-04-34-image.png)

就是如果没有预训练，直接训练中文QA然后测试中文QA，模型有78.1，但是如果做了预训练，后面训练英文的QA，机器在中文的QA上也能达到78.8分！证明机器通过学习了很多其他语言后，在一个语言上学习到某种能力后，也可以在其他语言上同样展现出这个能力。

 在机器的心理，所有人类的语言都是一种语言！![](images\2023-03-26-23-07-15-image.png)

#### ChatGPT增强式学习

用强化学习不需要给出答案，只需要告诉机器好不好就可以

- 人类老师可以偷懒

- 人类自己都不知道答案的时候，比如写诗，人类都不确定，让机器去写，你判定

<img src="file:///C:/Users/lyf/AppData/Roaming/marktext/images/2023-03-26-23-11-39-image.png" title="" alt="" data-align="left">

偶尔可以观察到前世的记忆，比如为什么这句话开头是一个逗号？就是这是一句话，它要把续写完，所以可以看到它前世GPT时期的表现。

![](images\2023-03-26-23-14-00-image.png)

### ChatGPT所带来的问题

#### 如何精准提出需求

需要进行催眠，也就是prompting，未来会有更系统的方案去做

![](images\2023-03-27-00-12-26-image.png)

![](images\2023-03-27-00-12-55-image.png)

#### 如何更正错误

ChatGPT的训练资料知道2021年

可能是人类老师训练的问题，它对2022也过于敏感了，只要提到2022就无法预测

![](images\2023-03-27-00-14-10-image.png)

![](images\2023-03-27-00-14-28-image.png)

如果它答错了能不能修正呢？有咩有可能修正了一个问题答案后，其他的问题答案也受到影响？就是Neural Editing，因为他背后都是神经网络~~

![](images\2023-03-27-00-16-07-image.png)

#### 侦测AI生成的物件

![](images\2023-03-27-00-17-10-image.png)

李宏毅的看法，我觉得有点站在精英角度看问题了。

![](images\2023-03-27-00-50-03-image.png)

#### 不小心泄露秘密

![](images\2023-03-27-00-51-17-image.png)

换一个角色扮演，绕着弯问他

![](images\2023-03-27-00-52-24-image.png)

这个问题是机器反学习，让他忘记曾经研究过的东西

![](images\2023-03-27-00-52-56-image.png)

## 用ChatGPT完成文字冒险游戏

##### 第一轮游戏

首先需要做一些prompting的设定

![](images\2023-03-27-00-54-56-image.png)

![](images\2023-03-27-00-55-19-image.png)

midjourney也需要好好地调教，这里衔接先人工搬运

![](images\2023-03-27-00-56-17-image.png)

midjourney 调教方法，但是它看英文会更好，所以我们可以让chatgpt生成英文

![](images\2023-03-27-00-56-42-image.png)

的确生成了游戏配图，但是不知道这个游戏名字是啥意思（笑

![](images\2023-03-27-00-57-58-image.png)

我们这里输入3

![](images\2023-03-27-00-58-29-image.png)

![](images\2023-03-27-00-58-47-image.png)

![](images\2023-03-27-00-59-33-image.png)

这里讨个巧，我们不用输入数字，打破常规，输入文字：直接进入遗迹

![](images\2023-03-27-01-00-17-image.png)

![](images\2023-03-27-01-00-57-image.png)

![](images\2023-03-27-01-12-21-image.png)

输入1，开始回答谜题

![](images\2023-03-27-01-01-19-image.png)

输入风，回答正确

下面这个图很有意思

![](images\2023-03-27-01-13-54-image.png)

输入稻草，回答正确！

![](images\2023-03-27-01-15-08-image.png)

输入云，答案对了

![](images\2023-03-27-01-16-14-image.png)

回到上一个，输入火，也依然对了！

![](images\2023-03-27-01-16-21-image.png)

我们强制游戏继续

![](images\2023-03-27-01-16-51-image.png)

![](images\2023-03-27-01-17-07-image.png)

![](images\2023-03-27-01-17-15-image.png)![](images\2023-03-27-01-17-49-image.png)很容易就被说服了，图中是2个人协商的样子![](images\2023-03-27-01-17-58-image.png)

最终游戏结束了

![](images\2023-03-27-01-18-22-image.png)

### 第二轮游戏

![](images\2023-03-27-01-19-18-image.png)

![](images\2023-03-27-01-19-40-image.png)

![](images\2023-03-27-01-19-56-image.png)

![](images\2023-03-27-01-20-21-image.png)

![](images\2023-03-27-01-20-33-image.png)

![](images\2023-03-27-01-20-54-image.png)

![](images\2023-03-27-01-21-06-image.png)

为什么是魔法海豹不是印章？因为seal有多义性

![](images\2023-03-27-01-21-44-image.png)

![](images\2023-03-27-01-22-21-image.png)

![](images\2023-03-27-01-22-30-image.png)

![](images\2023-03-27-01-22-45-image.png)

![](images\2023-03-27-01-22-54-image.png)

![](images\2023-03-27-01-23-14-image.png)

![](images\2023-03-27-01-23-20-image.png)

![](images\2023-03-27-01-23-28-image.png)

![](images\2023-03-27-01-23-37-image.png)

![](images\2023-03-27-01-23-53-image.png)

![](images\2023-03-27-01-24-13-image.png)

![](images\2023-03-27-01-24-22-image.png)

![](images\2023-03-27-01-24-41-image.png)

## 生成式学习策略

各个击破还是一步到位？

### 案例赏析

#### 生成文句

![](images\2023-03-28-01-20-16-image.png)

#### 生成影片

google的生成影片

![](images\2023-03-28-01-20-40-image.png)

![](images\2023-03-28-01-20-53-image.png)

#### 生成语音和声音

腾讯的一篇比较新的论文，语言合成的时候可以指定风格

![](images\2023-03-28-01-21-20-image.png)

可以捕捉很细微的差距，如周围环境

![](images\2023-03-28-01-25-25-image.png)

### 策略介绍

#### 策略1-各个击破

AR模型

![](images\2023-03-28-01-26-04-image.png)![](images\2023-03-28-01-26-16-image.png)

#### 策略2-一次到位

- 句子产生END，永远都输出固定长度，如果中间有END，后面就丢掉

- 也可以先生出数字，然后把100个字打出来

![](images\2023-03-28-01-27-59-image.png)

### 策略对比

![](images\2023-03-28-01-31-17-image.png)

#### 生成速度

影像由于像素点比较多，所以一般都使用一次到位的方式

![](images\2023-03-28-01-28-50-image.png)

#### 生成品质

各个击破会更好点

![](images\2023-03-28-01-30-53-image.png)

#### 中和方法

先有中间产物，决定好大方向，然后再一次到位的方法产生出来

![](images\2023-03-28-01-32-28-image.png)

从一次到位到N次到位![](images\2023-03-28-01-34-21-image.png)

## Finetuning VS Prompting

### 序言

语言模型中GPT是接龙，BERT是填空

![](images\2023-03-28-01-36-27-image.png)

大模型就像大象，需要我们做一点指引

下面图是用midjourney配上的

![](images\2023-03-28-01-36-53-image.png)

### 2种期待

#### 期待介绍

![](images\2023-03-28-01-38-16-image.png)

chatgpt就在走这个路线

![](images\2023-03-28-01-39-00-image.png)

其实很早2016年就有人写论文提出这个路线，他认为所有的自然语言处理都是问答的问题

给机器一篇文章，以及上下文，然后给一个答案

2015年的时候有更大的野心，认为所有的问题都可以用自然语言处理来解决

![](images\2023-03-28-01-41-44-image.png)

#### 期待对比

##### 专才好处

腾讯13年1月发了一篇文章，但是当时没有API，是手动输入的，所以每个任务就10个句子

![](images\2023-03-28-01-43-42-image.png)

12个翻译任务，分数越大越好，大多数情况下chatgpt都不如商用系统

![](images\2023-03-28-01-43-53-image.png)

后来chatgpt API有了之后，微软除了一篇文章，拿各种翻译比赛去对比

WMT-Best是比赛最强模型对比，chatgpt还是弱一点的

![](images\2023-03-28-01-45-16-image.png)

##### 通才好处

可以开发很快解决新场景问题

![](images\2023-03-28-01-47-07-image.png)

### 期待1改进

#### 增加外挂

BERT先天劣势就是不会讲话，只会填空，不会生成完整句子，所以需要加一个外挂

![](images\2023-03-28-01-48-26-image.png)

几个固定套路可改装BERT

![](images\2023-03-28-01-48-54-image.png)

#### 微调模型

将原本的参数作为初始化参数，然后更新参数

![](images\2023-03-28-01-50-06-image.png)

#### 增加适配器

还有一种方法增加插件，不改变原来的参数，而是增加layer

![](images\2023-03-28-01-50-51-image.png)

adapterhub有很多插件

![](images\2023-03-28-01-51-02-image.png)

用Adapter的好处是只要有对应adapter就好，可以共用一个很大的模型

![](images\2023-03-28-01-53-26-image.png)

### 期待2改进

用Instruction Learning和In-context Learning去训练

![](images\2023-03-28-01-54-58-image.png)

其实openai一直都想做这件事，2021年时GPT就想看题目叙述、再看范例就能回答问题

以为他们太狂了，要很久才能实现，没想到这么快

为什么GPT没这样做呢？李宏毅猜测可能一开始openai就有野望，也有可能是BERT已经尝试了，但是不行，所以另辟蹊径，这些未来都可以做故事，反正都成功了

![](images\2023-03-28-02-05-55-image.png)

#### 范例学习

##### 范例的真正作用

事先标注给机器学习，但是有人怀疑机器是否真的能读懂？所以故意标错误的结果去测试

![](images\2023-03-28-02-07-53-image.png)

试验结果表明，即使给了错误的范例，其表现也不给范例好，所以看起来没有从范例中真正的学习

![](images\2023-03-28-02-08-47-image.png)

继续另一个试验：随意sample句子当context，看结果如何

![](images\2023-03-28-02-10-14-image.png)

黄色给正确的句子，红色给错误的句子，紫色给无关的句子，蓝色不给

结果表明如果不给domin，其学习效果就很无效

![](images\2023-03-28-02-10-59-image.png)

这些例子证明了，语言模型本身就有做情感分析的能力了

只是去通过in-context learning的方式去唤醒它！

![](images\2023-03-28-02-13-29-image.png)

到8个的时候很快就收敛了，证明给范例的个数不是很重要，主要是唤醒记忆。下面从0一直增加到32，如果是fine-tuning，那么差距会非常大！

![](images\2023-03-28-02-14-16-image.png)

##### 范例学习的作用原理

范例也可以达成梯度下降的作用

![](images\2023-03-28-02-16-59-image.png)

##### 大模型受范例影响更大

横轴是给机器有多少例子是错误的，发现越是大型模型受到这个错误范例的影响越大，证明大模型才可以从范例中进行学习，比如PaLM参数越大下降的越快

前面3个模型超过50%错误的时候预测准确率低于50%，证明其可以在错误的语料学习

![](images\2023-03-28-02-18-05-image.png)

还有一个更疯狂的事情，让他读取一些无意义的东西，然后有输出，看看它是否会变成分类器，发现input的dimension到64的时候就比SVM差一点点

<img src="file:///C:/Users/lyf/AppData/Roaming/marktext/images/2023-03-28-02-22-42-image.png" title="" alt="" data-align="inline">

![](images\2023-03-28-02-23-06-image.png) 

##### 先具备范例学习能力

刚才的例子都没有让机器先学习做范例学习的能力，它还只是会文字接龙

下面这篇论文尝试了先让他具备这个学习能力，效果的确好很多

![](images\2023-03-28-02-26-37-image.png)

#### 题目学习

##### 指令微调

通过范例学习对人类还没有那么自然，毕竟还要找案例挺麻烦，能不能让机器看懂我们让他干什么？其实使用文字接龙模型是不够的，所以需要进行instruction-tuning

![](images\2023-03-28-19-51-24-image.png)

这个概念也不是全新的，21年有个T0模型，给他不同的指令让它做不同的任务，期待给他做自然语言推论，会给你一个正确的答案

![](images\2023-03-28-19-53-41-image.png)

还有一个21年的谷歌的模型，起手是刚开始收集NLP的任务+有标注的数据集

![](images\2023-03-28-19-54-34-image.png)

接下来要把这些任务改写成指令，假设现在做推论任务，先给前提再给假设让机器判定有没有矛盾。但是现在人类想让机器看懂你的指令，人们对每个NLP 的任务都想了不同的描述方式。总之就是想针对推论任务用不同的方式描述起来给机器

![](images\2023-03-28-20-33-39-image.png)

表现效果还可以，需要强调的是，如果要做的是NLI的任务，那么训练资料中就没有NLI

FLAN这种学过做的表现，是比GPT3的要好的

![](images\2023-03-28-20-34-33-image.png)

##### 思考链路

###### 人工标注

下面是另外一种prompting的方式。比如文献上发现说数学推理的问题，如果给in-context learning范例效果不是太好，CoT的概念是，给结果时顺便给一些推论过程，期待它看到新问题时会写出推论过程，从而给出答案。

![](images\2023-03-28-23-12-59-image.png)

文献上表现在数学题上表现的比较好。从18能提高到57。

![](images\2023-03-28-23-13-28-image.png)

###### Zero shot

但是有的时候没有解题的过程，机器在回答的时候增加一句：Let's think step by step.

![](images\2023-03-28-23-16-51-image.png)

让机器有多个回答，然后取一样的结果，大概就是正确的了

![](images\2023-03-28-23-19-07-image.png)

CoT这步其实对chatgpt无效，因为他本身就会输出过程，反而你需要抑制他这种能力

实验发现不做CoT反而更好，所以这也是为什么先有CoT，然后引入了不好的结果，然后self-consistency才被搭配了出来

![](images\2023-03-28-23-20-59-image.png)

数学问题太难了，所以需要拆解，比如把下面的问题先拆解出玩滑水道的时间+剩多少时间

所以这里还是得做一些in-context learning

![](images\2023-03-28-23-22-14-image.png)

### 用机器来找prompt

之前都是hard prompt，现在用soft的，就是向量也是可以训练的，可以理解成是一种adapter

![](images\2023-03-28-23-24-40-image.png)

还可以用强化学习

![](images\2023-03-28-23-26-12-image.png)

直接让LM想出自己的prompt，我们给I/O，让他自己想出来

![](images\2023-03-28-23-27-03-image.png)![](images\2023-03-28-23-27-39-image.png)

官方上有一些Prompting的方法![](images\2023-03-28-23-28-23-image.png)

# 图像生成模型

## 图像生成模型概览

### 图像生成的特点和问题

文字生成时候的可能性还比较明确，但是图像、语音生成都需要机器大量的脑补，这个导致模型的设计会有不同的方法

![](images\2023-03-28-23-30-29-image.png)

图像可以模仿NLP的AR方法，对颜色的生成可以使用逐个击破，颜色没必要要256\*\*3，可以就选择256色就可以了

![](images\2023-03-28-23-33-56-image.png)

Openai就有这样的api，故意做了动画表示这是一排一排生成出来的。这种方法可以生成很高清的图片，但是在影像生成中太耗费算力了

![](images\2023-03-28-23-34-34-image.png)

如果每个pixels都是独立的，那么不同pixel想画的狗不同，结果会很差

![](images\2023-03-28-23-36-36-image.png)

影响生成的模型都不是直接输出图，而是里面会有一个normal distribution，从你知道的分布中做一个sample。无论是VAE还是diffusion model，都是先用分布去sample。

原本的p(x|y)很难求，那么不妨就是sample出中间结果，然后用中间结果去对应到最终的图片上，所有的模型的想法都是一样的，解法是不同的！

![](images\2023-03-28-23-40-10-image.png)

### 常用的生成模型速览

#### VAE

变分自动编码器（Variational autoEncoder，VAE）

如何训练一个向量对应到一个图片的decoder呢？那么我们可以相反的把一个图进行encoder，然后两个对应上去同时训练，两个越接近越好。然后中间加个限制，强迫中间是正态分布的

![](images\2023-03-28-23-43-42-image.png)

#### Flow-based

去训练一个encoder，输出结果是正态分布，画图的时候反过来用

flow-based要刻意限制network的架构，才能让encoder可以反过来用

并且输入的图片要和输出的图片一样大的！否则不好invertible

![](images\2023-03-28-23-46-43-image.png)

#### Diffusion Model

一张图片不停的加噪声，一直加到正态分布

生成图片就用一个denoise的方法去去掉噪音，这个denoise其实就是decoder

![](images\2023-03-28-23-48-47-image.png)

#### GAN

只去训练decoder，不训练encoder

一开始输出是乱七八糟的东西，然后训练discriminator

如果discriminator的表现很差，就说明他们很接近

![](images\2023-03-28-23-51-17-image.png)

#### 总结

![](images\2023-03-28-23-52-35-image.png)

GAN和他们3个模型其实不一样并不互斥的，所以都可以加上去，下面是个15年的论文

![](images\2023-03-28-23-54-20-image.png)

## Diffusion Model原理

![](images\2023-03-28-23-54-56-image.png)

### Denoise训练

先sample出一个噪音，其尺寸和原图片一样，然后不断用denoise netword去denoise

![](images\2023-03-28-23-56-43-image.png)

其过程就如米开朗琪罗所说：雕像就在大理石里面，他只不过是把不要的部分拿掉了。

![](images\2023-03-28-23-57-55-image.png)

我们其实都在用一个denoise的model，但是它在吃一个额外的数，就是现在denoise的次数

![](images\2023-03-29-00-00-13-image.png)

denoise的内部是先预测出一个噪声，然后再减掉

产生带噪声的猫和噪声的难度是明显不同的

![](images\2023-03-29-00-02-04-image.png)

如何训练noise predictor？得告诉它这张图的噪声长什么样子

![](images\2023-03-29-00-02-36-image.png)

自己加噪声，之后你就有训练资料了

![](images\2023-03-29-00-03-44-image.png)

![](images\2023-03-29-00-04-23-image.png)

#### 文字转图片

imagenet有1M，但是Stable Diffusion/DALLE/Midjourney等基于的LAION，其有58亿的图片。训练资料既有中文又有英文，所以训练资料非常多。

![](images\2023-03-29-00-19-50-image.png)

每次denoise的模组都要加这个内容

![](images\2023-03-29-00-20-26-image.png)![](images\2023-03-29-00-20-50-image.png)

训练的时候也把文字输入带进来

![](images\2023-03-29-00-21-19-image.png)

![](images\2023-03-29-00-21-32-image.png)

## Stable Diffusion原理

### 3元件框架

现在最好的模型会有3个元件，1个encoder，1个生成模型，文字过去后会产生一个中间产物

然后再用这个中间产物还原，通常3个模型分开训练再组合

![](images\2023-03-29-00-23-06-image.png)![](images\2023-03-29-00-23-40-image.png)

DALL-E系列也是这个套路，用AR或者Diffusion生成中间结果

![](images\2023-03-29-00-24-12-image.png)

Google的是小图不断生成到大图，套路也是一样

![](images\2023-03-29-00-24-42-image.png)

### encoder

encoder的size影响会很大，FID值越小结果越好 CLIP SCORE是越大越好，越往右下角越好

上面的10k是sample了1万张

![](images\2023-03-29-00-27-16-image.png)

#### FID指标

如何评估图片生成模型的好坏，把图片丢进CNN后，出现representation

然后把真实的和生成的画在一起，如果这两组的representation很接近就很好

假设他们是高斯分布，然后计算Frechet distance，虽然有点粗糙，但是是好的

问题是sample的image要很多

![](images\2023-03-29-00-30-48-image.png)

#### CLIP指标

CLIP是400万个Image-text训练出来的模型

把你这次的文字和图片都丢进去，看他们的距离是否接近

![](images\2023-03-29-00-31-54-image.png)

### decoder

在训练的时候不需要 文字-图片，可以单凭图片资料就可以训练出来

![](images\2023-03-29-00-34-24-image.png)

因为中间产物是一张小图就sample

![](images\2023-03-29-00-34-59-image.png)

如果是latent-representation，就要训练一个auto-encoder，和VAE的那种感觉差不多

imagen是用小图当中间产物，stable diffusion和dall-e就是用latent representation

这个representation可以看成是中间小图，只是channel不同，人可能看不懂

![](images\2023-03-29-00-37-48-image.png)

### generation model

先拿encoder输出中间产物，然后在这个基础上增加噪声 

![](images\2023-03-29-00-39-03-image.png)

训练的时候文字和noise叠完后的结果去训练![](images\2023-03-29-00-40-43-image.png)

midjourney不是一开始噪声，而是一开始就是模糊的图，然后慢慢变清楚

这其实是因为他们把中间这些latent-representation去decoder后拿出来给你们看

![](images\2023-03-29-00-41-31-image.png)
